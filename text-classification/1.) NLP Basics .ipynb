{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text featurization and simplification using spaCy</h2>\n",
    "<p>There are many packages for doing NLP, particularly:<br/>\n",
    "<ul>\n",
    "<li><a href=\"https://spacy.io/\">spaCy</a></li>\n",
    "<li><a href=\"http://www.nltk.org/\">NLTK (Stanford)</a></li>\n",
    "<li><a href=\"https://github.com/mit-nlp/MITIE\">MITIE (MIT)</a></li>\n",
    "</ul>\n",
    "\n",
    "I'll be walking through spaCy and NLTK which is the most prevolent.  Let's start by loading our module.\n",
    "spaCy uses NN to perform all of the part of speech/lemetization/tokenization tasks that we hope to do.  The following call is reading in a set of weights (that need to be downloaded):<br/>\n",
    "```\n",
    "nlp = spacy.load('en')\n",
    "```\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Basic Clean Up</h2>\n",
    "We will start by reviewing the effects of the following steps:\n",
    "<ul>\n",
    "<li>Lowering \\& punctuation stripping</li>\n",
    "<li>Stemming</li>\n",
    "<li>Lemmatization</li>\n",
    "<li>Stop Word Removal</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=13, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data = dataset.data\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens:\n",
      " [\"Well i'm not sure about the story nad it did seem biased.\"]\n",
      "\n",
      "Word Tokens:\n",
      " [['Well', 'i', \"'m\", 'not', 'sure', 'about', 'the', 'story', 'nad', 'it', 'did', 'seem', 'biased', '.']]\n",
      "\n",
      "No stop word tokens:\n",
      " [['Well', \"'m\", 'sure', 'story', 'nad', 'seem', 'biased', '.']]\n",
      "\n",
      "No stop word tokens:\n",
      " [['well', \"'m\", 'sure', 'stori', 'nad', 'seem', 'bias', '.']]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "text to sentence tokens ->\n",
    "Not always nessisary, but certainly useful when we want to preserve \n",
    "some contextual ques.\n",
    "'''\n",
    "l_s = sent_tokenize(data[0])\n",
    "print(\"Sentence Tokens:\\n\",l_s[:2])\n",
    "\n",
    "tokens = list(map(word_tokenize, l_s))\n",
    "print(\"\\nWord Tokens:\\n\", tokens[:2])\n",
    "\n",
    "s_stop = set(stopwords.words())\n",
    "tokens_stop_free = [[word for word in sent if word not in s_stop] for sent in tokens]\n",
    "print(\"\\nNo stop word tokens:\\n\", tokens_stop_free[:2])\n",
    "\n",
    "st = PorterStemmer()\n",
    "stems = [[st.stem(word) for word in sent] for sent in tokens_stop_free]\n",
    "print(\"\\nNo stop word tokens:\\n\", stems[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorization</h2>\n",
    "<p>Basic vectorization using sklearn</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36008, 27476, 16546, 28724, 35309, 19850, 21661,  6175, 16885,\n",
       "       17780, 22208, 16568, 29102,  7165, 32431,  9608,  4301, 18960,\n",
       "       29679, 11106,  6040, 31583,  5926,  9204, 29673, 29057, 34703,\n",
       "       11567, 14137, 14350, 33554, 18368, 35470, 25282, 21231, 11803,\n",
       "       18665, 29036, 14135, 21515, 17294, 38004, 19576, 27819, 29228,\n",
       "       29725, 19582, 30569, 22786, 33080, 12220,  6935, 12059, 24262,\n",
       "       33307, 33831], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                stop_words='english')\n",
    "\n",
    "vectorized = tf_vectorizer.fit_transform(data)\n",
    "\n",
    "vectorized[0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#vectorized[0].indices\n",
    "print([ x for x in vectorized[0].toarray()[0] if x!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unfortunate'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names()[vectorized[0].indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10645192432513927, 0.13206693492719013, 0.13911073526391529, 0.078280425970325543, 0.1221234257718059, 0.15225085694513146, 0.09499209337222235, 0.14727716815782282, 0.11380286854613492, 0.11080947107018407, 0.10263239106092439, 0.063333491255439217, 0.10645192432513927, 0.10645192432513927, 0.13565443116491285, 0.13273216768757162, 0.067786925676237994, 0.076248650507555291, 0.13206693492719013, 0.074382503502486347, 0.12694299022735112, 0.11168121952486858, 0.16962115296576974, 0.15424819608892076, 0.20696272450507047, 0.3238693490546905, 0.095650721990963791, 0.10104064353212389, 0.1061163901077104, 0.069708577574150082, 0.079384274707155839, 0.39735285671770609, 0.16539097862634769, 0.12961744379182066, 0.074952211469356267, 0.098878623466974791, 0.13818581093036333, 0.099690370220266764, 0.077128900942245643, 0.0957467079600101, 0.16193467452734525, 0.097040876749444088, 0.098319373814489477, 0.13565443116491285, 0.15225085694513146, 0.13206693492719013, 0.11080947107018407, 0.091768395729998661, 0.094445213475850109, 0.16539097862634769, 0.067073921731267, 0.052862021397748718, 0.12961744379182066, 0.071765573203230903, 0.1302007879221212, 0.073145398435686126]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print([ x for x in tfidf[0].toarray()[0] if x!=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>\n",
    "<p>Spacey allows us to do some interesting things during the tokenizaiton process.   Particularly, we can do `part of speech (pos)' and lemitization (lemma_)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well well INTJ UH intj Xxxx True False\n",
      "i -PRON- PRON PRP nsubj x True True\n",
      "'m be VERB VBP ROOT 'x False False\n",
      "not not ADV RB neg xxx True True\n",
      "sure sure ADJ JJ acomp xxxx True False\n",
      "about about ADP IN prep xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "story story NOUN NN compound xxxx True False\n",
      "nad nad NOUN NN pobj xxx True False\n",
      "it -PRON- PRON PRP nsubj xx True True\n",
      "did do VERB VBD aux xxx True True\n",
      "seem seem VERB VB relcl xxxx True True\n",
      "biased biased ADJ JJ oprd xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "What what NOUN WP pobj Xxxx True False\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "I -PRON- PRON PRP nsubj X True False\n",
      "disagree disagree VERB VBP csubj xxxx True False\n",
      "with with ADP IN prep xxxx True True\n",
      "is be VERB VBZ ROOT xx True True\n",
      "your -PRON- ADJ PRP$ poss xxxx True True\n",
      "statement statement NOUN NN nsubj xxxx True False\n",
      "that that ADP IN mark xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "U.S. u.s. PROPN NNP compound X.X. False False\n",
      "Media media PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ acl xx True True\n",
      "out out ADV RB advmod xxx True True\n",
      "to to PART TO prep xx True True\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "ruin ruin NOUN NN dep xxxx True False\n",
      "Israels israels PROPN NNPS compound Xxxxx True False\n",
      "reputation reputation NOUN NN dobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "That that DET DT nsubj Xxxx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "rediculous rediculous ADJ JJ acomp xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "The the DET DT det Xxx True False\n",
      "U.S. u.s. PROPN NNP compound X.X. False False\n",
      "media media NOUN NN nsubj xxxx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "the the DET DT det xxx True True\n",
      "most most ADV RBS advmod xxxx True True\n",
      "pro pro ADJ JJ amod xxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "israeli israeli ADJ JJ amod xxxx True False\n",
      "media medium NOUN NNS attr xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "world world NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "Having have VERB VBG aux Xxxxx True False\n",
      "lived live VERB VBN advcl xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "Europe europe PROPN NNP pobj Xxxxx True False\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "I -PRON- PRON PRP nsubj X True False\n",
      "realize realize VERB VBP ROOT xxxx True False\n",
      "that that ADP IN mark xxxx True True\n",
      "incidences incidenc VERB VBZ nsubj xxxx True False\n",
      "such such ADJ JJ amod xxxx True True\n",
      "as as ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "one one NOUN NN pobj xxx True True\n",
      "described describe VERB VBN acl xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "letter letter NOUN NN pobj xxxx True False\n",
      "have have VERB VBP aux xxxx True True\n",
      "occured occur VERB VBN ccomp xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "The the DET DT det Xxx True False\n",
      "U.S. u.s. PROPN NNP compound X.X. False False\n",
      "media medium NOUN NNS nsubj xxxx True False\n",
      "as as ADP IN prep xx True True\n",
      "a a DET DT det x True True\n",
      "whole whole ADJ JJ pobj xxxx True True\n",
      "seem seem VERB VBP ROOT xxxx True True\n",
      "to to PART TO aux xx True True\n",
      "try try VERB VB xcomp xxx True False\n",
      "to to PART TO xcomp xx True True\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "ignore ignore VERB VB xcomp xxxx True False\n",
      "them -PRON- PRON PRP dobj xxxx True True\n",
      ". . PUNCT . punct . False False\n",
      "The the DET DT det Xxx True False\n",
      "U.S. u.s. PROPN NNP nsubj X.X. False False\n",
      "is be VERB VBZ aux xx True True\n",
      "subsidizing subsidize VERB VBG ROOT xxxx True False\n",
      "Israels israels PROPN NNPS compound Xxxxx True False\n",
      "existance existance NOUN NN dobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "the the DET DT det xxx True True\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "Europeans europeans PROPN NNPS nsubj Xxxxx True False\n",
      "are be VERB VBP conj xxx True True\n",
      "not not ADV RB neg xxx True True\n",
      "( ( PUNCT -LRB- punct ( False False\n",
      "at at ADP IN advmod xx True True\n",
      "least least ADJ JJS advmod xxxx True True\n",
      "not not ADV RB neg xxx True True\n",
      "to to ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "same same ADJ JJ amod xxxx True True\n",
      "degree degree NOUN NN pobj xxxx True False\n",
      ") ) PUNCT -RRB- punct ) False False\n",
      ". . PUNCT . punct . False False\n",
      "So so ADV RB advmod Xx True False\n",
      "I -PRON- PRON PRP nsubj X True False\n",
      "think think VERB VBP ROOT xxxx True False\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "that that ADJ WDT nsubj xxxx True True\n",
      "might may VERB MD aux xxxx True True\n",
      "be be VERB VB ccomp xx True True\n",
      "a a DET DT det x True True\n",
      "reason reason NOUN NN attr xxxx True False\n",
      "they -PRON- PRON PRP nsubj xxxx True True\n",
      "report report VERB VBP relcl xxxx True False\n",
      "more more ADV RBR advmod xxxx True True\n",
      "clearly clearly ADV RB advmod xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "atrocities atrocity NOUN NNS pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "\n",
      "\t \n",
      "\t SPACE _SP  \n",
      "\t False False\n",
      "What what NOUN WP attr Xxxx True False\n",
      "is be VERB VBZ csubj xx True True\n",
      "a a DET DT det x True True\n",
      "shame shame NOUN NN attr xxxx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "that that ADP IN mark xxxx True True\n",
      "in in ADP IN prep xx True True\n",
      "Austria austria PROPN NNP pobj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "daily daily ADJ JJ amod xxxx True False\n",
      "reports report NOUN NNS nsubj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "the the DET DT det xxx True True\n",
      "inhuman inhuman ADJ JJ amod xxxx True False\n",
      "acts act NOUN NNS pobj xxxx True False\n",
      "commited commit VERB VBN acl xxxx True False\n",
      "by by ADP IN agent xx True True\n",
      "Israeli israeli ADJ JJ amod Xxxxx True False\n",
      "soldiers soldier NOUN NNS pobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "the the DET DT det xxx True True\n",
      "blessing blessing NOUN NN conj xxxx True False\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "received receive VERB VBN acl xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "Government government PROPN NNP pobj Xxxxx True False\n",
      "makes make VERB VBZ ccomp xxxx True False\n",
      "some some DET DT nsubj xxxx True True\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "Holocaust holocaust PROPN NNP compound Xxxxx True False\n",
      "guilt guilt NOUN NN pobj xxxx True False\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "go go VERB VBP ccomp xx True True\n",
      "away away ADV RB advmod xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "After after ADV RB advmod Xxxxx True False\n",
      "all all ADV RB advmod xxx True True\n",
      ", , PUNCT , punct , False False\n",
      "look look VERB VB ROOT xxxx True False\n",
      "how how ADV WRB advmod xxx True True\n",
      "the the DET DT det xxx True True\n",
      "Jews jews PROPN NNPS nsubj Xxxx True False\n",
      "are be VERB VBP aux xxx True True\n",
      "treating treat VERB VBG ccomp xxxx True False\n",
      "other other ADJ JJ amod xxxx True True\n",
      "races race NOUN NNS dobj xxxx True False\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n",
      "when when ADV WRB advmod xxxx True True\n",
      "they -PRON- PRON PRP nsubj xxxx True True\n",
      "got get VERB VBD advcl xxx True False\n",
      "power power NOUN NN dobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "It -PRON- PRON PRP nsubj Xx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "unfortunate unfortunate ADJ JJ acomp xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "\n",
      " \n",
      " SPACE   \n",
      " False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NER</h2>\n",
    "<p>Spacey also allows for Named Entity Recognition (NER).  It specifically keys on nouns being capitolized, then uses contextual cues to try to ascribe what type of enity the object is.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 62 63 GPE\n",
      "U.S. 106 110 GPE\n",
      "Media 111 116 ORG\n",
      "\n",
      " 126 127 GPE\n",
      "Israels 132 139 GPE\n",
      "U.S. 176 180 GPE\n",
      "\n",
      " 189 190 GPE\n",
      "Europe\n",
      " 247 254 LOC\n",
      "\n",
      " 312 313 GPE\n",
      "U.S. 338 342 GPE\n",
      "\n",
      " 374 375 GPE\n",
      "U.S. 392 396 GPE\n",
      "Israels 412 419 GPE\n",
      "\n",
      " 437 438 GPE\n",
      "Europeans 438 447 NORP\n",
      "\n",
      " 501 502 GPE\n",
      "\n",
      " 556 557 GPE\n",
      "Austria 597 604 GPE\n",
      "\n",
      " 622 623 GPE\n",
      "Israeli 652 659 NORP\n",
      "\n",
      " 685 686 GPE\n",
      "Government 704 714 ORG\n",
      "the Holocaust guilt\n",
      " 729 749 ORG\n",
      "Jews 782 786 NORP\n",
      "\n",
      " 811 812 GPE\n",
      "\n",
      " 851 852 GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katy Perry 0 10 PERSON\n",
      "Juicy J 33 40 PERSON\n",
      "San Diego 69 78 GPE\n",
      "Apple 83 88 ORG\n"
     ]
    }
   ],
   "source": [
    "temp = \"Katy Perry has collaborated with Juicy J in the past in a concert in San Diego for Apple\"\n",
    "doc = nlp(temp)\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>TF-IDF</h1>\n",
    "\n",
    "\n",
    "$idf_{t} = \\log(\\frac{N}{n_t})$ \n",
    "\n",
    "\n",
    "or smoothed\n",
    "\n",
    "$idf_{t} = \\log(1 + \\frac{N}{n_t})$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
